name: Weekly Netflix ML Pipeline

on:
  # 1. Schedule the pipeline to run every Sunday at 00:00 UTC
  schedule:
    - cron: "0 0 * * 0"
  # 2. Allow manual run via GitHub UI
  workflow_dispatch:

env:
  # Set the S3 bucket name here
  S3_BUCKET_NAME: your-netflix-s3-bucket-name # <<< RENAME THIS!

jobs:
  run_data_pipeline:
    runs-on: ubuntu-latest

    # Define environment variables for AWS credentials using GitHub Secrets
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: us-east-1 # Change to your AWS region

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Create necessary directories
        run: |
          mkdir -p backend/data
          mkdir -p backend/models
          mkdir -p backend/predictions # This directory is still created but not used for prediction output

      - name: Run Data Scraping and Feature Engineering (main.py)
        # Assuming main.py is in the backend directory
        run: python backend/main.py

      - name: Run Model Training and Prediction (train_models.py)
        # This generates latest_prediction.json in backend/models/
        run: python backend/train_models.py

      - name: Upload Results to S3 (Including Prediction JSON)
        # Uses the s3_utils.py script for model/data, and inline boto3 for API payload
        run: |
          # The backend directory is the current working directory for the python script
          python - <<EOF
          import sys
          import os
          import boto3

          # This import is necessary for the model/data file uploads later
          sys.path.append('backend')
          from s3_utils import upload_files_to_s3

          # --- API PAYLOAD UPLOAD (critical for Lambda) ---

          # Set up Boto3 client using GitHub Action environment variables
          # NOTE: AWS CLI or Boto3 is pre-installed in the GitHub runner environment
          s3_client = boto3.client('s3')

          # <<< CORRECTED PATH >>>: The train_models.py saves the file here.
          local_path = 'backend/models/latest_prediction.json'
          s3_key_for_api = 'latest/latest_prediction.json'
          bucket_name = os.environ['S3_BUCKET_NAME']

          print(f"Uploading {local_path} to s3://{bucket_name}/{s3_key_for_api}")
          s3_client.upload_file(local_path, bucket_name, s3_key_for_api)
          print("Successfully updated API payload file.")

          # --- MODEL/DATA UPLOAD (using s3_utils) ---

          # List all files generated by the pipeline (excluding the prediction file which was just uploaded)
          model_files = [
              'backend/data/weekly_features.csv',
              'backend/models/logistic_regression_model.pkl',
              'backend/models/random_forest_model.pkl',
              'backend/models/xgboost_model.pkl',
              'backend/models/lightgbm_model.pkl',
              # The prediction JSON is already uploaded, but feature importance is still useful
              'backend/models/feature_importance.json',
          ]

          # upload_files_to_s3 uses file.name as the S3 key, placing them at the bucket root.
          upload_files_to_s3(os.environ['S3_BUCKET_NAME'], model_files)
          EOF
