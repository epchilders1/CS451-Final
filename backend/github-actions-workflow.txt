# .github/workflows/weekly_data_update.yml
# This workflow runs every Monday at 9 AM UTC to collect fresh data

name: Weekly Netflix Data Update

on:
  schedule:
    # Runs every Monday at 9:00 AM UTC (cron format: minute hour day month weekday)
    - cron: '0 9 * * 1'
  
  # Allow manual trigger for testing
  workflow_dispatch:

jobs:
  update-data:
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Check out your repository
      - name: Checkout repository
        uses: actions/checkout@v3
      
      # Step 2: Set up Python environment
      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      # Step 3: Cache pip dependencies for faster runs
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      # Step 4: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      # Step 5: Set up Kaggle credentials
      - name: Set up Kaggle API credentials
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          mkdir -p ~/.kaggle
          echo '{"username":"${{ secrets.KAGGLE_USERNAME }}","key":"${{ secrets.KAGGLE_KEY }}"}' > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json
      
      # Step 6: Run data collection script
      - name: Collect Netflix data
        run: |
          python scripts/collect_data.py
      
      # Step 7: Run preprocessing and feature engineering
      - name: Process and prepare features
        run: |
          python scripts/process_data.py
      
      # Step 8: Train/update ML model
      - name: Train churn prediction model
        run: |
          python scripts/train_model.py
      
      # Step 9: Upload data to S3 (or commit to repo)
      - name: Upload to AWS S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: us-east-1
        run: |
          pip install boto3
          python scripts/upload_to_s3.py
      
      # Alternative: Commit processed data back to repo (if not using S3)
      - name: Commit and push changes (alternative to S3)
        if: false  # Set to true if using git storage instead of S3
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add data/processed/
          git add models/
          git diff --quiet && git diff --staged --quiet || git commit -m "Weekly data update - $(date +'%Y-%m-%d')"
          git push
      
      # Step 10: Notify on failure (optional)
      - name: Notify on failure
        if: failure()
        run: |
          echo "⚠️ Weekly data update failed!"
          # Add Slack/Discord webhook notification here if needed

  # Optional: Separate job to update dashboard
  deploy-dashboard:
    needs: update-data
    runs-on: ubuntu-latest
    if: success()
    
    steps:
      - name: Trigger Render deployment
        run: |
          # Render auto-deploys on git push, or use deploy hook
          curl -X POST "${{ secrets.RENDER_DEPLOY_HOOK }}"